# Install AWS cli in ubuntu
sudo apt update 
sudo apt-get upgrade -y

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

# Install unzip

sudo apt install unzip -y
unzip awscliv2.zip
sudo ./aws/install

# AWS cli version check for confirmation
aws --version

# Install Terraform in Ubuntu

sudo apt-get update && sudo apt-get install -y gnupg software-properties-common

wget -O- https://apt.releases.hashicorp.com/gpg | \
gpg --dearmor | \
sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg > /dev/null

gpg --no-default-keyring \
--keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg \
--fingerprint

echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \
https://apt.releases.hashicorp.com $(lsb_release -cs) main" | \
sudo tee /etc/apt/sources.list.d/hashicorp.list

sudo apt update
sudo apt-get install terraform
terraform -help plan



To install an **Amazon EKS (Elastic Kubernetes Service)** cluster, you can follow the step-by-step guide outlined below. 
This process involves setting up an EKS cluster using the AWS Management Console, **eksctl** (a CLI tool for EKS management), 
and **kubectl** for interacting with the cluster.

### Prerequisites

1. **AWS CLI** installed and configured.
   - Install AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html
   - Configure AWS CLI:

aws configure

2. **eksctl** installed.
   - Install eksctl: https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html

   curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
   sudo mv /tmp/eksctl /usr/local/bin

3. **kubectl** installed.
   - Install kubectl: https://kubernetes.io/docs/tasks/tools/install-kubectl/
   
   For macOS:
   ```bash
   brew install kubectl
   ```

   For Ubuntu/Linux:
   ```bash
   sudo snap install kubectl --classic
   ```

---

### Step-by-Step Process to Create an EKS Cluster

#### Step 1: Create an EKS Cluster Using `eksctl`

The simplest way to create an EKS cluster is by using **eksctl**. This tool handles the provisioning of the control plane, worker nodes, and other configurations.

Run the following command to create an EKS cluster with default settings:

```bash
eksctl create cluster \
  --name <your-cluster-name> \
  --region <your-region> \
  --nodegroup-name standard-workers \
  --node-type t3.medium \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 4 \
  --managed
```

- `--name`: Specify the name of your cluster.
- `--region`: Choose the AWS region where you want to create your cluster.
- `--nodegroup-name`: Name for the worker node group.
- `--node-type`: Type of EC2 instance for your nodes (e.g., `t3.medium`).
- `--nodes`: Number of worker nodes to create initially.
- `--nodes-min`: Minimum number of worker nodes in the Auto Scaling group.
- `--nodes-max`: Maximum number of worker nodes in the Auto Scaling group.
- `--managed`: Specifies that this node group will be managed by EKS.

**Example:**
```bash
eksctl create cluster \
  --name my-cluster \
  --region us-west-2 \
  --nodegroup-name standard-workers \
  --node-type t3.medium \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 5 \
  --managed
```

The command will:
- Create an EKS control plane.
- Launch a managed node group (EC2 worker nodes).
- Automatically configure networking (VPC, subnets, security groups).

**Output:**
After the cluster is created, the output will show the EKS endpoint and status.

#### Step 2: Configure `kubectl` to Access the EKS Cluster

Once the cluster is created, you can configure `kubectl` to access it.

**Check cluster context:**
```bash
aws eks --region <your-region> update-kubeconfig --name <your-cluster-name>
```

This command updates your **kubeconfig** file with the details of the newly created EKS cluster. Now, you can use `kubectl` to interact with your cluster.

**Verify `kubectl` configuration:**
```bash
kubectl get svc
```

You should see the default Kubernetes services like `kubernetes` in the output.

#### Step 3: Deploy a Sample Application (Optional)

To test your cluster, you can deploy a sample application using a simple deployment and service. This helps ensure that the cluster is working as expected.

Create a deployment:
```bash
kubectl create deployment nginx --image=nginx
```

Expose the deployment as a service:
```bash
kubectl expose deployment nginx --port=80 --type=LoadBalancer
```

Check the service:
```bash
kubectl get svc
```

Once the external IP is assigned to the `nginx` service, you can access the NGINX default page by navigating to the IP address in your browser.

#### Step 4: Scaling Your EKS Cluster (Optional)

To scale your cluster manually:
```bash
eksctl scale nodegroup --cluster=<your-cluster-name> --name=<nodegroup-name> --nodes=4
```

This command scales the number of nodes in your node group to 4.

#### Step 5: Cleaning Up

To delete the EKS cluster when you no longer need it:
```bash
eksctl delete cluster --name <your-cluster-name> --region <your-region>
```

---

### Advanced Configurations

#### Option 1: Creating a Highly Available EKS Cluster

You can specify multiple subnets in different availability zones for high availability. Hereâ€™s how to create a highly available cluster:

```bash
eksctl create cluster \
  --name prod-cluster \
  --region us-east-1 \
  --zones us-east-1a,us-east-1b,us-east-1c \
  --nodegroup-name high-availability-workers \
  --node-type t3.medium \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 6 \
  --managed
```

#### Option 2: Enable Fargate for EKS

To use AWS Fargate for running serverless pods (no EC2 instances required), you can create an EKS cluster with Fargate:

```bash
eksctl create cluster \
  --name fargate-cluster \
  --region us-west-2 \
  --fargate
```

You can then schedule workloads to Fargate.

#### Option 3: Cluster Autoscaler

To automatically scale your nodes in and out based on resource requests and limits, you can install the **Cluster Autoscaler** using Helm.

1. **Install the Cluster Autoscaler**:
   ```bash
   helm repo add autoscaler https://kubernetes.github.io/autoscaler
   helm install cluster-autoscaler autoscaler/cluster-autoscaler \
     --set cloudProvider=aws \
     --set autoDiscovery.clusterName=<your-cluster-name>
   ```

2. **Configure Autoscaler Policies** in the `values.yaml` file for advanced scaling options.

---

### Conclusion

You have now successfully created an Amazon EKS cluster using **eksctl**, configured **kubectl** for access, and optionally deployed a sample application to test your cluster. You can further enhance your cluster by enabling features like Fargate, Cluster Autoscaler, and setting up advanced networking configurations.

