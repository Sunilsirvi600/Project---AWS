Setting up a **Kubernetes cluster on AWS EC2 instances** with 3 master nodes (control planes) and 2 worker nodes involves several steps, including provisioning EC2 instances, configuring networking, installing required tools, and setting up the cluster using **kubeadm**. This setup ensures high availability (HA) for the control plane and allows the worker nodes to handle workloads.

Here's a step-by-step guide to the entire process:

### **1. Provision AWS EC2 Instances**

#### **Step 1.1: Create 5 EC2 Instances**
- Create **5 EC2 instances** using the AWS Management Console, CLI, or Terraform:
  - 3 instances for **master/control plane nodes**.
  - 2 instances for **worker nodes**.
- Recommended EC2 instance type: **t2.medium** or **t3.medium** for the masters and workers.
- **Operating System**: Use a compatible Linux distribution, such as **Ubuntu 20.04** or **Amazon Linux 2**.

#### **Step 1.2: Configure Security Groups**
- **Allow traffic on these ports** in the security group for your Kubernetes cluster:
  - **6443**: Kubernetes API server (allow from all nodes).
  - **2379-2380**: etcd server client API (allow from all master nodes).
  - **10250**: Kubelet API (allow from master and worker nodes).
  - **10251**: Kube-scheduler (allow from master nodes).
  - **10252**: Kube-controller-manager (allow from master nodes).
  - **30000-32767**: NodePort services (allow external access to NodePort services).
  - **SSH (22)**: For accessing instances remotely.

#### **Step 1.3: Set Up SSH Access**
- SSH into each EC2 instance using the private key associated with the EC2 instances.
  
  ssh -i /path/to/key.pem ubuntu@<public-ip>
  
### **2. Prepare All Nodes (Masters and Workers)**

#### **Step 2.1: Update and Install Dependencies**
On all **5 nodes** (master and worker), install the necessary packages:

# Update the system
sudo apt-get update && sudo apt-get upgrade -y

# Install Docker
sudo apt-get install -y docker.io

# Enable and start Docker
sudo systemctl enable docker
sudo systemctl start docker

# Install kubeadm, kubelet, and kubectl
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

#### **Step 2.2: Disable Swap on All Nodes**
Kubernetes requires swap to be disabled on all nodes. Run the following command on all nodes (master and worker):

sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

### **3. Initialize the Kubernetes Control Plane**

#### **Step 3.1: Initialize the First Master Node**
On the **first master node**, initialize the control plane using `kubeadm`:

sudo kubeadm init --control-plane-endpoint "<load-balancer-DNS-or-IP>:6443" --upload-certs

Replace `<load-balancer-DNS-or-IP>` with the DNS or IP of the load balancer that will balance traffic between the control planes. If you don't have a load balancer, use the IP address of the first master node.

- This command will initialize the first master node, configure the control plane components (etcd, kube-apiserver, kube-controller-manager, kube-scheduler), and print out a **join command** and **certificate key** to be used by the other control planes and worker nodes.

#### **Step 3.2: Configure kubectl Access on the First Master Node**
Run the following commands to set up your `kubectl` access on the first master node:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#### **Step 3.3: Set Up a Network Plugin**
Kubernetes requires a **network plugin** (CNI). Install **Weave**, **Flannel**, **Calico**, or another plugin of your choice. For example, to install Weave, run this command:

kubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')

### **4. Join Additional Master Nodes**

#### **Step 4.1: Retrieve Join Command and Certificate Key**
From the **first master node**, retrieve the join command and certificate key for adding more control plane nodes:

kubeadm token create --print-join-command

You will get output similar to:

kubeadm join <load-balancer-DNS-or-IP>:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <certificate-key>

#### **Step 4.2: Join Second and Third Master Nodes**
On the **second** and **third master nodes**, run the `kubeadm join` command you retrieved from the first master node:

sudo kubeadm join <load-balancer-DNS-or-IP>:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <certificate-key>

This will add the second and third master nodes to the cluster, creating a **highly available control plane**.

### **5. Join Worker Nodes to the Cluster**

#### **Step 5.1: Retrieve the Join Command for Worker Nodes**
On any master node, run the following command to get the **join command** for the worker nodes:

kubeadm token create --print-join-command

You will get a join command similar to:

kubeadm join <load-balancer-DNS-or-IP>:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>

#### **Step 5.2: Join Worker Nodes**
On each **worker node**, run the join command retrieved from the master node:

sudo kubeadm join <load-balancer-DNS-or-IP>:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>

This command will make the worker nodes part of the Kubernetes cluster, allowing them to schedule and run workloads.

### **6. Verify the Cluster**

#### **Step 6.1: Check Nodes**
On the first master node, check the status of all the nodes to ensure they are `Ready`:

kubectl get nodes

You should see all **5 nodes** listed (3 master nodes and 2 worker nodes):

NAME               STATUS   ROLES    AGE   VERSION
master-node-1      Ready    master   10m   v1.24.0
master-node-2      Ready    master   8m    v1.24.0
master-node-3      Ready    master   6m    v1.24.0
worker-node-1      Ready    <none>   4m    v1.24.0
worker-node-2      Ready    <none>   4m    v1.24.0

#### **Step 6.2: Verify Pod Networking**
To ensure the worker nodes can communicate across the network, deploy a test pod and verify the networking works:

kubectl run nginx --image=nginx
kubectl get pods -o wide

Ensure the pod is running and accessible.

### **7. Load Balancer for High Availability (Optional)**

For **high availability**, it's recommended to set up a load balancer (like AWS ELB) in front of the master nodes to balance API requests. The load balancer DNS or IP should be used as the `--control-plane-endpoint` in the `kubeadm init` step.

### **Summary**

1. **Provision 5 EC2 instances** (3 masters and 2 workers) and configure security groups.
2. **Install Docker and Kubernetes** components (`kubeadm`, `kubelet`, `kubectl`) on all nodes.
3. **Initialize the first master node** with `kubeadm init`, then join the second and third master nodes.
4. **Join worker nodes** to the cluster using the `kubeadm join` command.
5. **Verify the cluster** is running and the nodes are `Ready` with `kubectl get nodes`.
6. Optionally, configure an **AWS ELB load balancer** for high availability of the control plane.

This setup gives you a Kubernetes cluster with 3 master nodes for high availability and 2 worker nodes for running applications.
